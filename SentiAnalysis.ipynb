{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "btan4740_COMP5046_Ass1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "outputId": "e88e3f9b-c256-42a0-f38e-a2048202b5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1vF3FqgBC1Y-RPefeVmY8zetdZG1jmHzT'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_train.csv')\n",
        "\n",
        "id = '1XhaV8YMuQeSwozQww8PeyiWMJfia13G6'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"imdb_train.csv\")\n",
        "df_test = pd.read_csv(\"imdb_test.csv\")\n",
        "\n",
        "reviews_train = df_train['review'].tolist()\n",
        "sentiments_train = df_train['sentiment'].tolist()\n",
        "reviews_test = df_test['review'].tolist()\n",
        "sentiments_test = df_test['sentiment'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(reviews_train))\n",
        "print(\"Testing set number:\",len(reviews_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set number: 25000\n",
            "Testing set number: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "To preprocess data, regrex expression is used for remove tags such as '\\<br />', etc. Besides, all characters are changed to their lower case which can reduce the repeatness of vocabulary. For tags including 'pos' and 'neg', the label encoder transforms them into 0 and 1. 0 and 1 make the correctness can be easily computed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "# remove tags such as '<br />' using regular expression\n",
        "reg1 = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "reg2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "def clean_reviews(reviews):\n",
        "  # reduce all letters to lower case\n",
        "  reviews = [reg1.sub(\"\", line.lower()) for line in reviews]\n",
        "  reviews = [reg2.sub(\" \", line) for line in reviews]\n",
        "  return reviews\n",
        "\n",
        "reviews_train_clean = clean_reviews(reviews_train)\n",
        "reviews_test_clean = clean_reviews(reviews_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_6BHbdxtxTo",
        "colab_type": "code",
        "outputId": "8ba13b6d-6e04-4288-a5b3-0f2ffb427005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(reviews_train_clean[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i enjoyed this movie havent seen andy griffith in ages and felt he fit this role perfectly ive associated him with comedy but am pleased to see that hes versatile i wasnt troubled that dottys anxiety disorder may not have been verbatim from a psychiatric textbook there are zillions of whatever phobias and neuroses and these can take on a broad variety of quantitative and qualitative forms she is clearly a sensitive with extra sensory powers as was understood by the local indians but not by any anglos it is not surprising that this character is vulnerable and nominally eccentric although this is taken to be a light family movie it is actually more sophisticated than it seems also hirams twist at the end came as a pleasant surprise to me and tied all the preceding action together in a bundle its fun to contemplate the possibility of such spiritual guidance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDM8vrkhPiCI",
        "colab_type": "code",
        "outputId": "19c8205b-5f8f-4fce-aefe-bf2216f88510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "label_encoder.fit(sentiments_train+sentiments_test)\n",
        "print(label_encoder.classes_)\n",
        "\n",
        "labels_train = label_encoder.transform(sentiments_train)\n",
        "labels_test = label_encoder.transform(sentiments_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neg' 'pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpDUb3K_Qj-u",
        "colab_type": "code",
        "outputId": "4cfeee0f-d291-4edf-ccd6-d2100ea9e93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(sentiments_train[:8])\n",
        "print(labels_train[:8])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pos', 'pos', 'pos', 'neg', 'neg', 'neg', 'pos', 'pos']\n",
            "[1 1 1 0 0 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctz2aYEeRRUn",
        "colab_type": "code",
        "outputId": "2b7356a1-ebb1-49a7-fc9c-7fb82990848a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "label_dict = {0: 'neg', 1: 'pos'}\n",
        "print(label_dict[0], label_dict[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neg pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "source": [
        "For word embeddings, word2vec with CBOW is implemented. One main reason to choose word2vec-CBOW is that it uses 'sum' on the results of embedding outputs, which would make it faster than skip-gram. Skip-gram may work better on some uncommon words, but the GPU resource is limited on Colab, saving time is significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "source": [
        "The techniques used on data preprocessing for word embeddings are tokenization, removing common stop words and punctuations. These techniques split the review into single words and make the vocabulary size smaller, which can save some time. In the mean time, a word list and a word-to-number dictionary are made for word embeding. Additionally, one-hot encoding would be excuted within the `nn.Embedding`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H36Z423ijv8N",
        "colab_type": "text"
      },
      "source": [
        "Both the training set and testing set are implemented to this model. In this case, the vocabulary becomes slightly lager and the size of data for model training is nearly doubled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "outputId": "fcc8ac9c-0b8d-48bf-d79b-18e315ce0d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords as sw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihpV_jMb3w1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(sw.words('english'))\n",
        "\n",
        "def tokenizer(review):\n",
        "    review = word_tokenize(review)\n",
        "    line = []\n",
        "    for word in review:\n",
        "        if word.isalpha() and word not in stop_words:\n",
        "            line.append(word)\n",
        "    \n",
        "    return line\n",
        "\n",
        "tokens_train = [tokenizer(w) for w in reviews_train_clean]\n",
        "tokens_test = [tokenizer(w) for w in reviews_test_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAYOFQD80LlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the word list\n",
        "word_list = []\n",
        "for tokens in tokens_train+tokens_test:\n",
        "    word_list += [w for w in tokens]\n",
        "\n",
        "# make a list of unique words\n",
        "word_list = list(set(word_list))\n",
        "word_list.sort() # avoid randomness\n",
        "\n",
        "# make a word to number dictionary\n",
        "voc_size = len(word_list)\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yl9y_e24Nj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make cbow\n",
        "window_size = 4 # set window size, 5 would be better but 4 already took a whole night to run\n",
        "def make_cbow(dataset):\n",
        "  cbow = []\n",
        "  for review in dataset:\n",
        "    for i in range(window_size, len(review)-window_size-1):\n",
        "      target = word_dict[review[i]]\n",
        "      context = review[i-window_size:i] + review[i+1:i+1+window_size]\n",
        "      context = [word_dict[w] for w in context]\n",
        "      cbow.append([context, target])\n",
        "  \n",
        "  return(cbow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUmRlgNcXU3O",
        "colab_type": "code",
        "outputId": "7d2467c8-4d5e-4a37-9141-cf8a8fe7c219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "cbow_set = make_cbow(tokens_train+tokens_test)\n",
        "print(cbow_set[:8])\n",
        "print(len(cbow_set))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[36244, 76130, 50913, 101512, 48419, 2007, 40268, 41548], 4155], [[76130, 50913, 101512, 4155, 2007, 40268, 41548, 97213], 48419], [[50913, 101512, 4155, 48419, 40268, 41548, 97213, 85759], 2007], [[101512, 4155, 48419, 2007, 41548, 97213, 85759, 58916], 40268], [[4155, 48419, 2007, 40268, 97213, 85759, 58916, 6459], 41548], [[48419, 2007, 40268, 41548, 85759, 58916, 6459, 22115], 97213], [[2007, 40268, 41548, 97213, 58916, 6459, 22115, 87736], 85759], [[40268, 41548, 97213, 85759, 6459, 22115, 87736, 101437], 58916]]\n",
            "5559092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "source": [
        "The dimentions of word embedding is set to 256, which is slightly smaller than sqrt(voc_size). To reduce the running time and make the best use of every data, the model would iterate through all the data for 1 run. Also, a smaller learning rate may reduce the final loss but the learning rate is set to 0.01 which may fit the data a bit faster than a smaller learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftE4Uu1FlIE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01 # could be a bit smaller\n",
        "embedding_dim = 256\n",
        "no_of_epochs = len(cbow_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVPue28ykptN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.cuda\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "  def __init__(self, voc_size, embedding_dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    # embedding layer\n",
        "    self.embeddings = nn.Embedding(voc_size, embedding_dim)\n",
        "    # 2 linear layers\n",
        "    self.linear1 = nn.Linear(embedding_dim, voc_size//embedding_dim)\n",
        "    self.linear2 = nn.Linear(voc_size//embedding_dim, voc_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embeds = sum(self.embeddings(x)).view(1,-1) # sum the embeds\n",
        "    x = self.linear1(embeds)\n",
        "    # use relu and log_softmax to normalize the outputs of 2 linear layer2\n",
        "    x = F.relu(x)\n",
        "    x = self.linear2(x)\n",
        "    output = F.log_softmax(x, dim=1) # ouput is a series of probility of each word\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYVXZAgnoS2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = './drive/My Drive/btan4740_word.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4bB79LPk9Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbow_model = CBOW(voc_size=voc_size+1, embedding_dim=embedding_dim).to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimiser = optim.SGD(cbow_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "outputId": "0f29edee-bf8f-49a3-b03f-b0a5dd8dce65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "total_loss = 0\n",
        "for epoch in range(0, no_of_epochs): # iterate through all the data, more runs could be better but more time it does require\n",
        "  inputs_torch = torch.LongTensor(cbow_set[epoch][0]).to(device)\n",
        "  label_torch = torch.LongTensor([cbow_set[epoch][1]]).to(device)\n",
        "\n",
        "  cbow_model.train()\n",
        "\n",
        "  optimiser.zero_grad()\n",
        "\n",
        "  output = cbow_model(inputs_torch)\n",
        "\n",
        "  #print(output)\n",
        "  #print(label_torch)\n",
        "  loss = criterion(output, label_torch)\n",
        "  total_loss += loss.item()\n",
        "\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "  if epoch % 250000  == 249999: \n",
        "      print('Epoch: %d, loss: %.4f' %(epoch + 1, total_loss/250000))\n",
        "      total_loss = 0\n",
        "      torch.save(cbow_model, PATH)\n",
        "\n",
        "print('Epoch: %d, loss: %.4f' %(epoch + 1, loss))\n",
        "print('training finished')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 750000, loss: 9.0099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CBOW. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1000000, loss: 9.0541\n",
            "Epoch: 1250000, loss: 9.0165\n",
            "Epoch: 1500000, loss: 8.9920\n",
            "Epoch: 1750000, loss: 8.9760\n",
            "Epoch: 2000000, loss: 8.9545\n",
            "Epoch: 2250000, loss: 8.9169\n",
            "Epoch: 2500000, loss: 8.9302\n",
            "Epoch: 2750000, loss: 8.8976\n",
            "Epoch: 3000000, loss: 8.9340\n",
            "Epoch: 3250000, loss: 8.9030\n",
            "Epoch: 3500000, loss: 8.9062\n",
            "Epoch: 3750000, loss: 8.8865\n",
            "Epoch: 4000000, loss: 8.9186\n",
            "Epoch: 4250000, loss: 8.8779\n",
            "Epoch: 4500000, loss: 8.8488\n",
            "Epoch: 4750000, loss: 8.8495\n",
            "Epoch: 5000000, loss: 8.8406\n",
            "Epoch: 5250000, loss: 8.8387\n",
            "Epoch: 5500000, loss: 8.8296\n",
            "Epoch: 5559092, loss: 6.6299\n",
            "training finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QSpuVljrc6L",
        "colab_type": "code",
        "outputId": "02bf1f67-8ce8-4c9b-c155-7551f53b459e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "torch.save(cbow_model, PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CBOW. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "outputId": "26b2c747-de04-4c78-f9ef-527fe6d9ab16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "cbow_model2 = torch.load(PATH)\n",
        "cbow_model2 = cbow_model2.to(device)\n",
        "cbow_model2.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOW(\n",
              "  (embeddings): Embedding(130260, 256)\n",
              "  (linear1): Linear(in_features=256, out_features=508, bias=True)\n",
              "  (linear2): Linear(in_features=508, out_features=130260, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Data Preprocessing for Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AKbLnN-3GlI1"
      },
      "source": [
        "For regularizing the input, the length of word is set to 16, which means every single word length will be transformed to exactlly 16. Due to the gates of LSTM, some important previous imformation inputed will not be easily 'forget' like RNN. Thanks to this feature, adding paddings after (not before) the characters of some words does not have much negative effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i2CUCL1cGlI2",
        "outputId": "12b13622-b9d9-450b-e3b5-3af68ca1f64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# make char table: character->number\n",
        "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
        "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
        "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
        "            'v', 'w', 'x', 'y', 'z']\n",
        "# the number start from '1' cuz '0' is took for err handling\n",
        "char_dict = {n: i+1 for i, n in enumerate(char_arr)}\n",
        "char_dict_size = len(char_dict)\n",
        "\n",
        "# make train data: 16->1\n",
        "char_emb_train = []\n",
        "for word in word_list:\n",
        "  char_seq = []\n",
        "  # make sure all input size is exactly 16\n",
        "  for char in word:\n",
        "    if char in char_dict:\n",
        "      char_seq.append(char_dict[char])\n",
        "    else:\n",
        "      # if char is not a english character, put a '0' instead\n",
        "      char_seq.append(0)\n",
        "\n",
        "    if len(char_seq) >= 16:\n",
        "      break\n",
        "  # fill '0' into char_seq until its length become 16\n",
        "  if len(char_seq) < 16:\n",
        "    temp_seq = [0] * (16 - len(char_seq))\n",
        "    char_seq = char_seq + temp_seq \n",
        "    # putting '0's before the char_seq may be better for LSTM\n",
        "    # since the imformation could be 'forget' (actually it's a drawback of RNN, not LSTM)\n",
        "    # but the result turned out to be not really bad\n",
        "    # one reason could be the 'gates' of LSTM, the previous important imformation is remembered\n",
        "    # and also, 16 is not really long :D\n",
        "    # so just let it be like this\n",
        "\n",
        "  char_emb_train.append([char_seq, word_dict[word]])\n",
        "\n",
        "print(char_emb_train[:8])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0], [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 8, 8, 8], 2], [[1, 1, 1, 1, 1, 1, 1, 1, 18, 7, 8, 0, 0, 0, 0, 0], 3], [[1, 1, 1, 1, 1, 1, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0], 4], [[1, 1, 1, 1, 1, 1, 1, 8, 8, 8, 8, 8, 8, 7, 7, 7], 5], [[1, 1, 1, 1, 1, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0], 6], [[1, 1, 1, 1, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u13l7N6SXo57",
        "colab_type": "code",
        "outputId": "8c9028d1-7e43-4ae0-f049-d00f660595b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# make batch: randomly choose data from dataset\n",
        "def char_encoder(char_input):\n",
        "  temp = [0] * (char_dict_size + 1)\n",
        "  temp[char_input] = 1\n",
        "  return temp\n",
        "\n",
        "# make a dictionary for character vector\n",
        "char_vec = {char_dict[c]: char_encoder(char_dict[c]) for c in char_arr}\n",
        "char_vec[0] = [1] + ([0] * char_dict_size)\n",
        "print(char_vec)\n",
        "\n",
        "# randomly choose data as member of batch\n",
        "def batch_prep(dataset, batch_size):\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "  random_index = np.random.choice(range(len(dataset)), batch_size, replace=True)\n",
        "\n",
        "  for i in random_index:\n",
        "    input_batch.append([char_vec[c] for c in dataset[i][0]])\n",
        "    target_batch.append(dataset[i][1])\n",
        "\n",
        "  return np.array(input_batch), np.array(target_batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 2: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 3: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 4: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 5: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 6: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 7: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 8: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 10: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 11: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 12: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 13: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 14: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 15: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 16: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 17: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 18: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 19: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 20: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 21: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 22: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 23: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 24: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], 25: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 26: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgiOPcsTGlI6"
      },
      "source": [
        "### 2.2.2. Build Character Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4NtqFFcjGlI7"
      },
      "source": [
        "LSTM is used here for character embedding, as mentioned before, the feature of long-short term memmory does help to reduce the influence of vanishing gradient problem. As for hyperparameters, because this model runs fast, the learning rate is set to 0.005, and the epoch is a bit large which is related to the size of batch and dataset (basicly each characters-to-word set will be trained 32 times on average). The predicting accuracy turned to be satisfying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj3YZ3PWGlI8",
        "colab": {}
      },
      "source": [
        "# Setting hyperparameters\n",
        "batch_size = 512\n",
        "learning_rate = 0.005\n",
        "n_hidden = 256\n",
        "total_epoch = int(len(char_emb_train) / batch_size * 32)\n",
        "\n",
        "# Number of sequences (length of word)\n",
        "n_step = 16\n",
        "\n",
        "# number of inputs (dimension of input vector) = 27 (one more for err)\n",
        "n_input = 27\n",
        "# number of classes\n",
        "n_class = voc_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFQ8_EcpzPL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True)\n",
        "    self.linear = nn.Linear(n_hidden, n_class)\n",
        "\n",
        "  def forward(self, x):        \n",
        "    # lstm layer\n",
        "    x,_ = self.lstm(x)\n",
        "    # linear layer\n",
        "    x = self.linear(x[:,-1,:]) # only take the last state as prediction\n",
        "    # softmax layer\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "46W0zFfWGlI_"
      },
      "source": [
        "### 2.1.4. Train Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17eibm_4z8dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFAeDcYu0NeB",
        "colab_type": "code",
        "outputId": "ed142440-5872-4f02-c58c-1803f5ccfae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "for epoch in range(total_epoch):\n",
        "\n",
        "  input_batch, target_batch = batch_prep(char_emb_train, batch_size)\n",
        "  input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "  target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "  net.train()\n",
        "  outputs = net(input_batch_torch)\n",
        "  loss = criterion(outputs, target_batch_torch)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if epoch % 1000 == 999:\n",
        "    net.eval()\n",
        "    outputs = net(input_batch_torch) \n",
        "    train_loss = criterion(outputs, target_batch_torch)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    train_acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f, train_acc: %.4f'%(epoch + 1, train_loss.item(), train_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1000, train loss: 6.91362, train_acc: 0.0918\n",
            "Epoch: 2000, train loss: 1.76951, train_acc: 0.6152\n",
            "Epoch: 3000, train loss: 0.52696, train_acc: 0.8594\n",
            "Epoch: 4000, train loss: 0.21255, train_acc: 0.9570\n",
            "Epoch: 5000, train loss: 0.19624, train_acc: 0.9570\n",
            "Epoch: 6000, train loss: 0.20900, train_acc: 0.9590\n",
            "Epoch: 7000, train loss: 0.13306, train_acc: 0.9746\n",
            "Epoch: 8000, train loss: 0.09582, train_acc: 0.9766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R5Bym9bBGlJE"
      },
      "source": [
        "### 2.1.5. Save Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbFlt9qpZqrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH2 = './drive/My Drive/btan4740_character.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ggTsYIm7GlJF",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "#torch.save(net, PATH2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JwOI-wIKGlJI"
      },
      "source": [
        "### 2.1.6. Load Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-jyj-lOHWWj",
        "colab_type": "code",
        "outputId": "c6e1c2b9-d3d9-45c3-b586-db25512d7f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "char_emb_model = torch.load(PATH2)\n",
        "char_emb_model = char_emb_model.to(device)\n",
        "char_emb_model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (lstm): LSTM(27, 256, batch_first=True)\n",
              "  (linear): Linear(in_features=256, out_features=130259, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJAyFwAOEGGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_batch, _ = batch_prep(char_emb_train, 1)\n",
        "input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "output = char_emb_model(input_batch_torch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msIB9x4NJ5Uk",
        "colab_type": "code",
        "outputId": "822620fd-6502-4d44-ca07-2440e8b0b3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_, pred = torch.max(output, 1)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([100476], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h-xPjFtnK_A",
        "colab_type": "text"
      },
      "source": [
        "The sequence model is basicly a LSTM with an embedding layer. This model directly copy the embedding layer of previous word embedding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Character Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWM8uOWZqETq",
        "colab_type": "text"
      },
      "source": [
        "The character embedding model is applied as translator to predict the words from both training set and test set. And each review are changed to exactly 256-word long (adding empty words before reviews or cutting down the sentences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VjKOXHn3SGE",
        "colab_type": "text"
      },
      "source": [
        "Why not transform all reviews to number sequences and then generate batches through `Dataloader`? It could be done if there is no disconection issue (wasted a lot of time on it) and this would save a lot of training time. If the code ran locally, I will do this (actually I could have done the prediction locally and upload the processed data but I did not realized it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set max length\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "def word_pred(word):\n",
        "  vec_temp = []\n",
        "  #print(word)\n",
        "  # transform characters into numbers\n",
        "  # if not character, just put char_vec[0] instead\n",
        "  for ch in word:\n",
        "    if ch in char_arr:\n",
        "      vec_temp.append(char_vec[char_dict[ch]])    \n",
        "    else:\n",
        "      vec_temp.append(char_vec[0])\n",
        "\n",
        "    if len(vec_temp) >= 16:\n",
        "      break\n",
        "  # regularize the length of every word to 16\n",
        "  if len(vec_temp) < 16:\n",
        "    for i in range(16-len(vec_temp)):\n",
        "      vec_temp.append(char_vec[0])\n",
        "\n",
        "  vec_temp = np.array([vec_temp])\n",
        "  #print(vec_temp)\n",
        "\n",
        "  # predict the word through character embedding model\n",
        "  input_torch = torch.from_numpy(vec_temp).float().to(device)\n",
        "  output = char_emb_model(input_torch)\n",
        "  _, pred = torch.max(output, 1)\n",
        "  pred = pred.cpu().tolist()\n",
        "\n",
        "  return pred[0]\n",
        "\n",
        "def padding_filter(line, voc_size, max_length):\n",
        "  line_temp = []\n",
        "  for i in range(max_length - len(line)):\n",
        "    line_temp.append(voc_size)\n",
        "\n",
        "  return line_temp+line\n",
        "\n",
        "# transform review into a sequence of numbers predicted by character embedding model\n",
        "def seq_prep(review, voc_size, max_length):\n",
        "  line = []\n",
        "  for word in review:\n",
        "    pred = word_pred(word)\n",
        "    line.append(pred)\n",
        "    \n",
        "    if len(line) >= max_length:\n",
        "      break\n",
        "  \n",
        "  if len(line) < max_length:\n",
        "    line = padding_filter(line, voc_size, max_length)\n",
        "\n",
        "  return line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhFD2k812uYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_prep(tokens_train[0], voc_size, MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_hxVONcTIRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare batch\n",
        "# choose randoml reviews as inputs\n",
        "def batch_prep2(dataset_X, dataset_y, batch_size):\n",
        "  inputs_batch = []\n",
        "  labels_batch = []\n",
        "  random_index = np.random.choice(range(len(dataset_X)), batch_size, replace=True)\n",
        "\n",
        "  for i in random_index:\n",
        "    inputs_batch.append(seq_prep(dataset_X[i], voc_size, MAX_LENGTH))\n",
        "    labels_batch.append(dataset_y[i])\n",
        "\n",
        "  return inputs_batch, labels_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U7SW-YBz7il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_prep2(tokens_train, labels_train, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4",
        "colab_type": "text"
      },
      "source": [
        "The learning rate is 0.001 according to the total epoch and batch_size. For each epoch, a batch of 128 reviews would be randomly selected from the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZyfF4c_0o-5",
        "colab_type": "text"
      },
      "source": [
        "The main difference between the sequence model and the character embedding model is the embedding layer copied from word2vec-CBOW model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9xUPZ4dOzE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "tagset_size = 2\n",
        "hidden_dim = 256\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dYkskQlH-LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeqTagger(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(SeqTagger, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    # embedding layer, the size is the same as word2vec-CBOW model\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # LSTM\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True) \n",
        "    # LSTM to tag\n",
        "    self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds)\n",
        "    # only select the last state of LSTM\n",
        "    tag_space = self.hidden2tag(lstm_out[:,-1,:])\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEH5S8ZO9yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_model = SeqTagger(embedding_dim, hidden_dim, voc_size+1, tagset_size).to(device)\n",
        "seq_model.word_embeddings = cbow_model2.embeddings\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(seq_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.3. Train Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab_type": "code",
        "outputId": "11959c53-3a3e-4375-a80f-97d275304d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "loss_now = 0.0\n",
        "for epoch in range(400):  \n",
        "  # total epoch could be larger if more time and GPU resources were available\n",
        "  X, y = batch_prep2(tokens_train, labels_train, batch_size)\n",
        "  sentences = torch.LongTensor(X).to(device)\n",
        "  labels = torch.tensor(y).view(-1).to(device)\n",
        "\n",
        "  seq_model.zero_grad()\n",
        "  seq_model.train()\n",
        "  tag_scores = seq_model(sentences)\n",
        "  \n",
        "  # loss calculate and backward\n",
        "  loss = loss_function(tag_scores, labels.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  loss_now+=loss.item()\n",
        "  \n",
        "  if epoch % 20 == 19: \n",
        "    seq_model.eval()\n",
        "    tag_scores = seq_model(sentences)\n",
        "    _, predicted = torch.max(tag_scores, 1)\n",
        "    train_acc= accuracy_score(predicted.cpu().numpy(), labels.view(-1).cpu().numpy())\n",
        "    # print the average loss of last 20 epoch and the accuracy of this epoch\n",
        "    print('Epoch: %d, training loss: %.4f, training acc: %.2f'%(epoch+1, loss_now/20, train_acc))\n",
        "    loss_now = 0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 260, training loss: 0.0805, training acc: 0.74\n",
            "Epoch: 280, training loss: 0.5093, training acc: 0.73\n",
            "Epoch: 300, training loss: 0.4410, training acc: 0.80\n",
            "Epoch: 320, training loss: 0.3671, training acc: 0.79\n",
            "Epoch: 340, training loss: 0.3301, training acc: 0.83\n",
            "Epoch: 360, training loss: 0.3230, training acc: 0.92\n",
            "Epoch: 380, training loss: 0.2873, training acc: 0.88\n",
            "Epoch: 400, training loss: 0.2873, training acc: 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.4. Save Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH3 = './drive/My Drive/btan4740_sequence.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnZmO7O1FdnS",
        "colab_type": "code",
        "outputId": "286d2982-f109-40a2-b6f6-1ea9c515680a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "torch.save(seq_model, PATH3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type SeqTagger. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.5. Load Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_model2 = torch.load(PATH3)\n",
        "seq_model2 = seq_model2.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Performance Evaluation\n",
        "\n",
        "\n",
        "A table with precision, recall, f1 of test set is provided and one batch of 1280 random cases (not the whole set, mainly for save time because the sequence generator runs really slow) from test set are selected for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = batch_prep2(tokens_test, labels_test, 1280)\n",
        "sentences = torch.LongTensor(X).to(device)\n",
        "labels = torch.tensor(y).view(-1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV1M1Fw4O2x5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_model2.eval()\n",
        "\n",
        "tag_scores = seq_model2(sentences)\n",
        "_, predicted = torch.max(tag_scores, 1)\n",
        "y_pred = predicted.cpu().numpy()\n",
        "y_true = labels.view(-1).cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXPrJH_mNDEJ",
        "colab_type": "code",
        "outputId": "7aa1f0de-777b-46ff-b533-3f08e03aed5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "\"\"\"\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\"\"\"\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_true, y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8337    0.6305    0.7180       636\n",
            "           1     0.7059    0.8758    0.7817       644\n",
            "\n",
            "    accuracy                         0.7539      1280\n",
            "   macro avg     0.7698    0.7531    0.7498      1280\n",
            "weighted avg     0.7694    0.7539    0.7500      1280\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Hyperparameter Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F83Lr68DKcj_",
        "colab_type": "text"
      },
      "source": [
        "The batch size is different from training process (128->1280), so the epoch is reduced. And the learning rate is the same as it in training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(seq_model2.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w026fg7cLCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1s = []\n",
        "for epoch in range(20):  \n",
        "  X, y = batch_prep2(tokens_test, labels_test, 1280)\n",
        "  sentences = torch.LongTensor(X).to(device)\n",
        "  labels = torch.tensor(y).view(-1).to(device)\n",
        "\n",
        "  seq_model2.zero_grad()\n",
        "  seq_model2.train()\n",
        "  tag_scores = seq_model2(sentences)\n",
        "  \n",
        "  # loss calculate and backward\n",
        "  loss = loss_function(tag_scores, labels.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  seq_model2.eval()\n",
        "  tag_scores = seq_model2(sentences)\n",
        "  _, predicted = torch.max(tag_scores, 1)\n",
        "  y_pred = predicted.cpu().numpy()\n",
        "  y_true = labels.view(-1).cpu().numpy()\n",
        "  # get f1 scores\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  f1s.append((epoch, f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9cachG3CZmS",
        "colab_type": "code",
        "outputId": "79949126-3a4a-4293-df9d-17fb6a4c61ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(f1s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.7669819432502148), (1, 0.7567114093959731), (2, 0.7818028643639426), (3, 0.8438880706921944), (4, 0.8395802098950526), (5, 0.8302493966210781), (6, 0.8283642224012894), (7, 0.839622641509434), (8, 0.8305489260143198), (9, 0.845872899926954), (10, 0.8524833209785027), (11, 0.8595548733691482), (12, 0.845360824742268), (13, 0.8601798855273917), (14, 0.8477034649476229), (15, 0.8514548238897396), (16, 0.8487261146496815), (17, 0.8661764705882353), (18, 0.8639344262295081), (19, 0.8704422032583398)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9bNuOKNBUJ-",
        "colab_type": "code",
        "outputId": "7eb60875-4569-472e-aa50-f311cfecee89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import BSpline\n",
        "%matplotlib inline\n",
        "\n",
        "Xs = []\n",
        "Ys = []\n",
        "for point in f1s:\n",
        "  x, y = point\n",
        "  Xs += [x]\n",
        "  Ys += [y]\n",
        "\n",
        "Xs = np.array(Xs, dtype=int)\n",
        "Ys = np.array(Ys)\n",
        "\n",
        "plt.figure()\n",
        "plt.xlim((0, 18))\n",
        "plt.ylim((0.75, 0.9))\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('f1 score')\n",
        "plt.plot(Xs, Ys)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Iy7lTfKvMk",
        "colab_type": "text"
      },
      "source": [
        "The figure shows that with the learning rate 0.001, since epoch 11 the f1 socre tends to be stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhisT7xmSd6T",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "The model could be improved (hyperparameters, network size); the preprocessing techniques could be furtherly optimized, such as merging different tenses for the same word; and some mistakes, such as adding padding after the characters in character embedding, should be avoid."
      ]
    }
  ]
}